# -*- coding: utf-8 -*-
"""AI_CENSUS_INCOME_ANALYSIS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P2_E3pg5LrHm0ELaw6dhuoHoTJceiLup

#Census Income Analysis

### Context

According to the government, census income is the income received by an individual regularly before payments for personal income taxes, medicare deductions, and so on. This information is asked annually from the people to record in the census. It helps to identify the eligible families for various funds and programs rolled out by communities and the government.

### Problem Statement

The dataset is extracted from 1994 Census Bureau. The data includes an instance of anonymous individual records with features like work-experience, age, gender, country, and so on. Also have divided the records into two labels with people having a salary **more than 50K or less than equal to 50K** so that they can determine the eligibility of individuals for government opted programs.

Looks like a very interesting dataset and as a data scientist, your job is to build a prediction model to predict whether a particular individual has an annual income of **<=50k** or **>50k**.

**Things To Do:**

1. Importing and Analysing the Dataset

2. Data Cleaning

3. Feature Engineering

4. Train-Test Split

5. Features Selection Using RFE

6. Model Training and Prediction Using Ideal Features

### Dataset Description

The dataset includes 32560 instances with 15 features and 1 target column which can be briefed as:

|Field|Description|
|---:|:---|
|age|age of the person, Integer|
|work-class| employment information about the individual, Categorical|
|fnlwgt| unknown weights, Integer|
|education| highest level of education obtained, Categorical|
|education-years|number of years of education, Integer|
|marital-status| marital status of the person, Categorical|
|occupation|job title, Categorical|
|relationship| individual relation in the family-like wife, husband, and so on. Categorical|
|race|Categorical|
|sex| gender, Male or Female|
|capital-gain| gain from sources other than salary/wages, Integer|
|capital-loss| loss from sources other than salary/wages, Integer|
|hours-per-week| hours worked per week, Integer|
|native-country| name of the native country, Categorical|
|income-group| annual income, Categorical,  **<=50k** or **>50k** |


**Notes:**
1. The dataset has no header row for the column name. (Can add column names manually)
2. There are invalid values in the dataset marked as **"?"**.
3. As the information about **fnlwgt** is non-existent it can be removed before model training.
4. Take note of the **whitespaces (" ")**  throughout the dataset.\

#### Activity 1:  Importing and Analysing the Dataset

In this activity, we have to load the dataset and analyse it.


**Perform the following tasks:**
- Load the dataset into a DataFrame.
- Rename the columns with the given list.
- Verify the number of rows and columns.
- Print the information of the DataFrame.


**1.** Start with importing all the required modules:

**Note**: Also import the `warnings` module and include `warnings.filterwarnings('ignore')` to skip the unnecessary warnings.
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
from plotly.offline import iplot
import plotly as py
py.offline.init_notebook_mode(connected=True)
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
import warnings
warnings.filterwarnings('ignore')

ai = pd.read_csv("https://raw.githubusercontent.com/dsrscientist/dataset1/master/census_income.csv")

ai.head()

"""Rename the columns by applying the `rename()` function using the following column list:

>```python
column_name =['age', 'workclass', 'fnlwgt', 'education', 'education-years', 'marital-status', 'occupation', 'relationship', 'race','sex','capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income-group']
```

"""

# Rename the column names in the DataFrame using the list given above.

# Create the list
column_name =['age', 'workclass', 'fnlwgt', 'education', 'education-years', 'marital-status', 'occupation', 'relationship', 'race','sex','capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income-group']
old_name = list(ai.columns)
column_dict = {}
for i in old_name:
  for j in column_name:
    column_dict[i] = j
    column_name.remove(j)
    break

# Rename the columns using 'rename()'
ai.rename(column_dict,axis=1,inplace=True)

# Print the first five rows of the DataFrame
ai.head()

ai.info()

# Print the number of rows and columns of the DataFrame
print(f'Number of rows = {ai.shape[0]}')
print(f'Number of columns = {ai.shape[1]}')

ai.info()

# Check the distribution of the labels in the target column.
ai['income-group'].value_counts()

"""#### Activity 2: Data Cleaning


In this activity, we need to clean the DataFrame step by step.

**Perform the following tasks:**
- Check for the null or missing values in the DataFrame.
- Observe the categories in column `native-country`, `workclass`, and `occupation`.
- Replace the invalid `" ?"` values in the columns with `np.nan` using `replace()` function.
- Drop the rows having `nan` values using the `dropna()` function.



**1.** Verify the missing values in the DataFrame:
"""

# Check for null values in the DataFrame.
ai.isnull().sum()

# Print the distribution of the columns mentioned to find the invalid values.

# Print the categories in column 'native-country'
print("The categories in column 'native-country'")
print('-'*50)
print(ai['native-country'].unique(), '\n'*2)

# Print the categories in column 'workclass'
print("The categories in column 'workclass'")
print('-'*50)
print(ai['workclass'].unique(), '\n'*2)

# Print the categories in column 'occupation'
print("The categories in column 'occupation'")
print('-'*50)
print(ai['occupation'].unique(), '\n'*2)

# Replace the invalid values ' ?' with 'np.nan'.
ai.replace(to_replace={' ?': np.nan}, inplace=True)

# Check for null values in the DataFrame again.
ai.isnull().sum()

# Delete the rows with invalid values and the column not required

# Delete the rows with the 'dropna()' function
ai = ai.dropna()
# Delete the column with the 'drop()' function
ai.drop(columns=['fnlwgt'],inplace=True)

# Print the number of rows and columns in the DataFrame.
print('The number of rows in the dataframe is', ai.shape[0])
print('The number of columns in the dataframe is', ai.shape[1])

# a quick look on some statistics about the data
ai.describe()

# Commented out IPython magic to ensure Python compatibility.
# Heat map
import matplotlib.pyplot as plt
import seaborn as sb

# %matplotlib inline
plt.figure(figsize=[10,10])

ct_counts = ai.groupby(['education-years', 'income-group']).size()
ct_counts = ct_counts.reset_index(name = 'count')
ct_counts = ct_counts.pivot(index = 'education-years', columns = 'income-group', values = 'count').fillna(0)

sb.heatmap(ct_counts, annot = True, fmt = '.0f', cbar_kws = {'label' : 'Number of Individuals'})
plt.title('Number of People for Education Class relative to Income')
plt.xlabel('Income ($)')
plt.ylabel('Education Class');

# Clustered Bar Chart
plt.figure(figsize=[8,6])
ax = sb.barplot(data = ai, x = 'income-group', y = 'age', hue = 'sex')
ax.legend(loc = 8, ncol = 3, framealpha = 1, title = 'Sex')
plt.title('Average of Age for Sex relative to Income')
plt.xlabel('Income ($)')
plt.ylabel('Average of Age');

# Bar Chart
plt.figure(figsize=[8,6])
sb.barplot(data=ai, x='income-group', y='hours-per-week', palette='YlGnBu')
plt.title('Average of Hours per Week relative to Income')
plt.xlabel('Income ($)')
plt.ylabel('Average of Hours per Week');

"""#### Activity 3: Feature Engineering

The dataset contains certain features that are categorical.  To convert these features into numerical ones, use the `map()` and `get_dummies()` function.


**Perform the following tasks for feature engineering:**

- Create a list of numerical columns.

- Map the values of the column `gender` to:
  - **`Male: 0`**
  - **`Female: 1`**

- Map the values of the column `income-group` to:
  - **` <=50K: 0`**
  - **` >50K: 1`**

- Create a list of categorical columns.

- Perform **one-hot encoding** to obtain numeric values for the rest of the categorical columns.

---

**1.**  Separate the numeric columns first for that create a list of numeric columns using `select_dtypes()` function:
"""

# Create a list of numeric columns names using 'select_dtypes()'.
numeric_df = ai.select_dtypes(include=['int64','float64'])
numeric_columns = list(ai.select_dtypes(include = ['int64', 'float64']).columns)

# Map the 'sex' column and verify the distribution of labels.

# Print the distribution before mapping
print('Before mapping')
print('-'*50)
print(ai['sex'].value_counts())

# Map the values of the column to convert the categorical values to integer
ai['sex'] = ai['sex'].map({' Male':0,' Female':1})

# Print the distribution after mapping
print('\nAfter mapping')
print('-'*50)
print(ai['sex'].value_counts())

# Map the 'income-group' column and verify the distribution of labels.

# Print the distribution before mapping
print('Before mapping')
print('-'*50)
print(ai['income-group'].value_counts())

# Map the values of the column to convert the categorical values to integer
ai['income-group'] = ai['income-group'].map({' <=50K' : 0, ' >50K' : 1})

# Print the distribution after mapping
print('\nAfter mapping')
print('-'*50)
print(ai['income-group'].value_counts())

# Create the list of categorical columns names using 'select_dtypes()'.
categorical_col = list(ai.select_dtypes(include=['object']).columns)
categorical_col

# Create a 'income_dummies_df' DataFrame using the 'get_dummies()' function on the non-numeric categorical columns
income_dummies_df = pd.get_dummies(ai[categorical_col], drop_first=True, dtype=int)
income_dummies_df

"""#### Activity 4: Train-Test Split

We need to predict the value of the `income-group` variable, using other variables. Thus, `income-group` is the target or dependent variable and other columns except `income-group` are the features or the independent variables.

**1.** Split the dataset into the training set and test set such that the training set contains 70% of the instances and the remaining instances will become the test set.
"""

X = ai.drop(['income-group'],axis=1)
y = ai['income-group']

X_train, X_test, y_train, y_test  = train_test_split(X,y, test_size=0.30, random_state=11)

lr=LogisticRegression() #Logistic Regression
rf=RandomForestClassifier() # Random Forest
svm=SVC()              # support vactor classifier
xgboost=XGBClassifier()  #Xtrim Gredient Boosting Classifier
print("Model is created")

# Create a 'income_dummies_df' DataFrame using the 'get_dummies()' function on the non-numeric categorical columns
income_dummies_df = pd.get_dummies(ai[categorical_col], drop_first=True, dtype=int)

# Concatenate the dummy variables with the numeric features
X = pd.concat([ai[numeric_columns], income_dummies_df], axis=1)
y = ai['income-group']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.30, random_state=11)

# Now you can fit the models
lr.fit(X_train, y_train)
rf.fit(X_train,y_train)
svm.fit(X_train,y_train)
xgboost.fit(X_train,y_train)
print("Model is trained")

print("Lr classification score",lr.score(X_train,y_train))
print("rf classification score",rf.score(X_train,y_train))
print("svm classification score",svm.score(X_train,y_train))
print("xgboost classification score",xgboost.score(X_train,y_train))

lr_yprad = lr.predict(X_test)
rf_yprad = rf.predict(X_test)
svm_yprad = svm.predict(X_test)
xgboost_yprad = xgboost.predict(X_test)

lr_conf_mat = confusion_matrix(y_test,lr_yprad)
print("confusion matrix for lr_model",'\n',lr_conf_mat)

rf_conf_mat = confusion_matrix(y_test,rf_yprad)
print("confusion matrix for lr_model",'\n',rf_conf_mat)

svm_conf_mat = confusion_matrix(y_test,svm_yprad)
print("confusion matrix for svm_model",'\n',svm_conf_mat)

xgboost_conf_mat = confusion_matrix(y_test,xgboost_yprad)
print("confusion matrix for xgboost_model",'\n',xgboost_conf_mat)

"""####Activity 5: Features Selection for all models

Feature selection is the process of selecting a subset of relevant features from the original dataset. It helps to reduce the dimensionality of the data, simplify the model, improve model performance, and reduce overfitting.
"""

from sklearn.feature_selection import SelectKBest, f_classif

# Separate features and target variable
X = ai.drop('income-group', axis=1)
y = ai['income-group']

# Convert categorical features to numeric using one-hot encoding
X = pd.get_dummies(X, drop_first=True)

# Feature Selection using SelectKBest and f_classif (ANOVA F-value)
bestfeatures = SelectKBest(score_func=f_classif, k='all')  # Use 'all' to score all features
fit = bestfeatures.fit(X, y)
dfscores = pd.DataFrame(fit.scores_)
dfcolumns = pd.DataFrame(X.columns)

# Concatenate columns
feature_score = pd.concat([dfcolumns, dfscores], axis=1)
feature_score.columns = ['Feature', 'Score']  # Name the columns
print("Feature Scores: \n", feature_score.nlargest(15, 'Score'))  # Print top 15 features

# Threshold-based selection (example: keep features with scores above a threshold)
threshold = 100  # You can adjust this threshold
important_features = feature_score[feature_score['Score'] > threshold]['Feature'].tolist()
print("\nImportant Features: \n", important_features)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Scale numerical features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Models
logistic_model = LogisticRegression(solver='liblinear', random_state=42)
rf_model = RandomForestClassifier(random_state=42)
svm_model = SVC(random_state=42)
xgb_model = XGBClassifier(random_state=42)

models = {
    "Logistic Regression": logistic_model,
    "Random Forest": rf_model,
    "SVM": svm_model,
    "XGBoost": xgb_model
}

for name, model in models.items():
    print(f"--- {name} ---")

    # Train and predict using all features
    model.fit(X_train_scaled, y_train)
    y_pred_all = model.predict(X_test_scaled)
    accuracy_all = accuracy_score(y_test, y_pred_all)
    print(f"Accuracy (All Features): {accuracy_all:.4f}")

    # Train and predict using selected features
    if important_features:
        X_train_selected = X_train_scaled[:, [X.columns.get_loc(col) for col in important_features]]
        X_test_selected = X_test_scaled[:, [X.columns.get_loc(col) for col in important_features]]
        model.fit(X_train_selected, y_train)
        y_pred_selected = model.predict(X_test_selected)
        accuracy_selected = accuracy_score(y_test, y_pred_selected)
        print(f"Accuracy (Selected Features): {accuracy_selected:.4f}")

    print(classification_report(y_test, y_pred_all))

"""####Activity 6: Model Training and Prediction Using Ideal Features"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report

# Define the target variable and features
X = ai.drop('income-group', axis=1)
y = ai['income-group']

# Identify categorical columns for one-hot encoding
categorical_cols = [col for col in X.columns if X[col].dtype == 'object']

# Perform one-hot encoding
X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)

# Select "ideal" features (replace with your actual list)
# ideal_features = ['age', 'workclass', 'fnlwgt', 'education', 'education-years', 'marital-status', 'occupation', 'relationship', 'race','sex','capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income-group']
# Instead of ideal_features, use all columns in X after one-hot encoding
ideal_features = X.columns  # Use all columns present after one-hot encoding

# Filter the features (This line is now redundant as ideal_features includes all columns)
#X = X[ideal_features]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Scale the numerical features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Define and train the models
logistic_model = LogisticRegression(solver='liblinear', random_state=42)
rf_model = RandomForestClassifier(random_state=42)
svm_model = SVC(random_state=42)
xgb_model = XGBClassifier(random_state=42)

logistic_model.fit(X_train, y_train)
rf_model.fit(X_train, y_train)
svm_model.fit(X_train, y_train)
xgb_model.fit(X_train, y_train)

# Make predictions
logistic_pred = logistic_model.predict(X_test)
rf_pred = rf_model.predict(X_test)
svm_pred = svm_model.predict(X_test)
xgb_pred = xgb_model.predict(X_test)

# Evaluate the models
print("Logistic Regression Accuracy:", accuracy_score(y_test, logistic_pred))
print("Random Forest Accuracy:", accuracy_score(y_test, rf_pred))
print("SVM Accuracy:", accuracy_score(y_test, svm_pred))
print("XGBoost Accuracy:", accuracy_score(y_test, xgb_pred))

print("\nLogistic Regression Classification Report:\n", classification_report(y_test, logistic_pred))
print("\nRandom Forest Classification Report:\n", classification_report(y_test, rf_pred))
print("\nSVM Classification Report:\n", classification_report(y_test, svm_pred))
print("\nXGBoost Classification Report:\n", classification_report(y_test, xgb_pred))

"""## Activity 7: Creating front end interface using ****GRADIO****"""

!pip install gradio scikit-learn pandas joblib

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
import joblib

# Load your dataset
ai = pd.read_csv("https://raw.githubusercontent.com/dsrscientist/dataset1/master/census_income.csv")

# Assign column names based on your PDF
ai.columns = [
    "age", "workclass", "fnlwgt", "education", "education-num", "marital-status",
    "occupation", "relationship", "race", "sex", "capital-gain", "capital-loss",
    "hours-per-week", "native-country", "income"
]

# Drop columns not used (e.g., 'fnlwgt', 'education-num', etc.)
ai = ai.drop(["fnlwgt", "education-num", "capital-gain", "capital-loss"], axis=1)

# Clean whitespaces
ai = ai.applymap(lambda x: x.strip() if isinstance(x, str) else x)

# Binary target
ai["income"] = ai["income"].apply(lambda x: 1 if x == ">50K" else 0)

X = ai.drop("income", axis=1)
y = ai["income"]

# Categorical columns
cat_cols = X.select_dtypes(include="object").columns.tolist()
num_cols = X.select_dtypes(include="number").columns.tolist()

# Preprocessing pipeline
preprocessor = ColumnTransformer([
    ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols)
], remainder='passthrough')

# Model pipeline
clf = Pipeline([
    ("pre", preprocessor),
    ("rf", RandomForestClassifier(n_estimators=100, random_state=42))
])

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
clf.fit(X_train, y_train)

# Save the full pipeline
joblib.dump(clf, "income_pipeline.pkl")

import gradio as gr
import joblib
import pandas as pd

# Load pipeline
pipeline = joblib.load("income_pipeline.pkl")

# Prediction function
def predict_income(age, workclass, education, marital_status, occupation,
                   relationship, race, sex, hours_per_week, native_country):
    input_dict = {
        'age': [age],
        'workclass': [workclass],
        'education': [education],
        'marital-status': [marital_status],
        'occupation': [occupation],
        'relationship': [relationship],
        'race': [race],
        'sex': [sex],
        'hours-per-week': [hours_per_week],
        'native-country': [native_country]
    }
    input_df = pd.DataFrame(input_dict)
    prediction = pipeline.predict(input_df)
    return ">50K" if prediction[0] == 1 else "<=50K"

demo = gr.Interface(
    fn=predict_income,
    inputs=[
        gr.Number(label="Age"),
        gr.Dropdown(["Private", "Self-emp-not-inc", "Local-gov", "State-gov", "Federal-gov", "Other"], label="Workclass"),
        gr.Dropdown(["Bachelors", "HS-grad", "Some-college", "Masters", "Assoc-voc", "Other"], label="Education"),
        gr.Dropdown(["Never-married", "Married-civ-spouse", "Divorced", "Separated", "Widowed"], label="Marital Status"),
        gr.Dropdown(["Tech-support", "Craft-repair", "Sales", "Exec-managerial", "Prof-specialty", "Other"], label="Occupation"),
        gr.Dropdown(["Husband", "Not-in-family", "Own-child", "Unmarried", "Other"], label="Relationship"),
        gr.Dropdown(["White", "Black", "Asian-Pac-Islander", "Amer-Indian-Eskimo", "Other"], label="Race"),
        gr.Radio(["Male", "Female"], label="Sex"),
        gr.Number(label="Hours per Week"),
        gr.Dropdown(["United-States", "Mexico", "Philippines", "Germany", "Other"], label="Native Country")
    ],
    outputs="text",
    title="Census Income Classifier",
    description="Predict if income >50K/year based on U.S. Census Data"
)

demo.launch(share=True)  # `share=True` creates a public link for testing

"""####Conclusion

In this project, we aimed to predict individual income levels using the 1994 Census Bureau data.  Several machine learning models were evaluated, including Logistic Regression, Random Forest, SVM, and XGBoost.  A user-friendly Gradio interface was developed to facilitate model interaction and prediction.

**Model Performance Summary**

Logistic Regression achieved an accuracy of 84% and an F1-score of 0.84.

The Random Forest model attained an accuracy of 84% with an F1-score of 0.84.

SVM demonstrated an accuracy of 85% and an F1-score of 0.84.

XGBoost outperformed the other models, reaching an accuracy of 87% and an F1-score of 0.86.

**Key Findings**

XGBoost consistently showed the best performance across all metrics, indicating its effectiveness in accurately predicting income levels.  The Gradio application provides a simple way for users to input data and receive predictions from the trained models.

**Future Work**

Further improvements could involve additional feature engineering, hyperparameter tuning, and exploring other advanced models.  Additionally, analyzing the impact of specific features on income prediction would provide valuable insights.  Expanding the Gradio interface with more interactive visualizations and user-friendly features would also be beneficial.
"""